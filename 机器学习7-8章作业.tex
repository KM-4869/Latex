\documentclass{ctexart}

\usepackage{enumerate}
\usepackage{amsmath}%bmatrix矩阵
\usepackage{tabularray}%表格
\usepackage{float}%浮动环境
\usepackage{graphicx}%图像
\usepackage{bm}
\usepackage{geometry}
\usepackage{booktabs}%三条表格划线命令

\geometry{a4paper,scale=0.8}
\begin{document}

\section*{第7章《维度归约》习题}
\subsection*{名词解释}
\begin{enumerate}[(1)]
\item 欧氏距离\\
连续d维空间中的两个向量的欧氏距离表示为
\begin{equation*}
L_2(\bm{x}_i,\bm{x}_j)=\sqrt{\sum\limits^d\limits_{l=1}|x_{il}-x_{jl}|^2}
\end{equation*}
\item 马氏距离\\
设样本的协方差矩阵为$\Sigma$,则两个样本之间的马氏距离为：
\begin{equation*}
D_M(\bm{x}_i,\bm{x}_j)=\sqrt{(\bm{x}_i-\bm{x}_j)^T\Sigma^{-1}(\bm{x}_i-\bm{x}_j)}
\end{equation*}
\end{enumerate}


\subsection*{降维分析中涉及投影的变换矩阵通常要求是正交的。请问（1）正交投影矩阵相对于非正交投影矩阵用于降维的优点是什么？（2）为什么在PCA中变换矩阵一定是正交的？}
\begin{enumerate}[(1)]
\item 正交矩阵可以使降维后的每个样本维数之间是不相关的，降低特征之间的耦合性。
\item 因为样本的协方差阵总是实对称矩阵，对于不同特征值的特征向量总是正交的，总是可以找到正交矩阵用于矩阵的对角化。
\end{enumerate}

\subsection*{参照教材图6-11，绘制一个二维数据集示例，让PCA和LDA找到相同的好方向。另绘制一个数据集，让PCA和LDA都找不到一个好的投影方向}

\begin{figure}[H]
\includegraphics[height=9.5cm,width=1.0\linewidth]{图一.jpg}
\caption{PCA和LDA找到相同的好方向}
\end{figure}

\begin{figure}[H]
\includegraphics[height=10cm,width=1.0\linewidth]{图二.jpg}
\caption{PCA和LDA都找不到一个好的投影方向}
\end{figure}

\subsection*{计算题：用PCA方法计算这6个二维样本数据的一维PCA主分量，并计算该主分量的方差贡献率}
\begin{equation*}
\begin{bmatrix}
2&3&3&4&5&7\\
2&4&5&5&6&8
\end{bmatrix}
\end{equation*}

中心化样本后得$\bm{X}$：
\begin{equation*}
\begin{bmatrix}
-2&-1&-1&0&1&3\\
-3&-1&0&0&1&3\\
\end{bmatrix}
\end{equation*}

样本协方差矩阵为$\bm{C}$:
\begin{equation*}
C=\frac{1}{6}\bm{X}\bm{X}^T=\begin{bmatrix}
\dfrac{8}{3}&\dfrac{17}{6}\\
&\\
\dfrac{17}{6}&\dfrac{10}{3}
\end{bmatrix}
\end{equation*}

求协方差矩阵的特征值与对应的特征向量：
\begin{gather*}
det(\lambda\bm{I}-\bm{C})=0\\
\left|\begin{matrix}
\lambda-\dfrac{8}{3}&-\dfrac{17}{6}\\
&\\
-\dfrac{17}{6}&\lambda-\dfrac{10}{3}
\end{matrix}\right|=0\\
\end{gather*}

\begin{gather*}
(\lambda-\dfrac{8}{3})(\lambda-\dfrac{10}{3})-\left(\dfrac{17}{6}\right)^2=0\\
\lambda^2-6\lambda+\dfrac{31}{9}=0\\
\end{gather*}
解得$\lambda_1=0.1475$,$\lambda_2= 5.8525$,对应的单位化后的特征向量为$\bm{x}_1=\left(\begin{array}{c}-0.747\\0.664\end{array}\right)$,$\bm{x}_2=\left(\begin{array}{c}0.664\\ 0.747\end{array}\right)$.

故一维主分量为5.8525.方差贡献率为

\begin{equation*}
\dfrac{5.8525}{5.8525+0.1475}=0.975
\end{equation*}

\section*{第8章《聚类》习题}
\subsection*{名词解释}
\begin{enumerate}[(1)]
\item Jaccard系数\\
是一种聚类结果评价的常用外部指标。通过对样本集中聚类给出的簇和参考模型给出的簇划分，把样本集两两配对并统计相应的簇相同或相异的样本对数。其中Jaccard系数定义如下
\begin{equation*}
JC=\frac{a}{a+b+c}
\end{equation*}
\item 密度可达\\
在DBSCAN聚类方法中，若$x_i$能通过若干个密度直达的点对$x_j$密度直达，则称$x_j$由$x_i$密度可达。
\end{enumerate}

\subsection*{k-means算法与kNN方法的k分别指什么？这两个方法的主要区别是什么？}
k-means算法中的k指事先人为设定样本集应聚为k个类别，kNN方法中的k是指在新样本的周围通过某种度量选取k个与之距离最近的k个点，用这k个点的均值或出现最多类别作为新样本的分类或回归依据。

主要区别为k-means算法是一种无监督的聚类方法，用于将相似的数据聚在一起。kNN是一种有监督的分类方法，用于实现从已有的数据中学习出对新样本类别的预测。

\subsection*{给定含有5个样本的集合X=$\begin{bmatrix}
0&0&1&5&5\\2&0&0&0&2
\end{bmatrix}$,试用k-means聚类算法将样本聚到两个类中(k=2),选$(0,0)^T$,$(5,2)^T$为初始质心}
计算剩余3个样本分别到两个质心的距离的平方：
\begin{align*}
d_{12}=4&&d_{15}=25\\
d_{32}=1&&d_{35}=20\\
d_{42}=25&&d_{45}=4
\end{align*}
故可将样本分为两簇$C_1={1,2,3},C_2={4,5}$,重新计算中心向量有$\mu_1=\left(\begin{array}{c}0.333\\0.666\end{array}\right),\mu_2=\left(\begin{array}{c}5\\1\end{array}\right)$.
\begin{align*}
d_{1,\mu_1}=1.890&&d_{1,\mu_2}=26\\
d_{2,\mu_1}=0.554&&d_{2,\mu_2}=26\\
d_{3,\mu_1}=0.942&&d_{3,\mu_2}=17\\
d_{4,\mu_1}=22.224&&d_{4,\mu_2}=1\\
d_{5,\mu_1}=23.560&&d_{5,\mu_2}=1\\
\end{align*}

此时新的分类结果不变，则聚类过程结束。两簇样本分别为$C_1={1,2,3},C_2={4,5}$。

\subsection*{对于下图的数据分类，宜采用什么类聚类算法？其基本原理是什么？该算法中有哪两个重要参数？这两个参数的取值对聚类的结果有什么影响？}

\begin{figure}[H]
\centerline{\includegraphics[height=10cm,width=0.8\linewidth]{图三.jpg}}
\caption{第八章第五题图}
\end{figure}

可以采用DBSCAN算法。它首先初始任意选择一个核心对象，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止

重要参数为$\epsilon-$领域与MinPts。$\epsilon-$领域衡量了一个样本能够包含周围的范围大小。若$\epsilon$值选取较大，则每个样本领域内都能包含较多的其他样本，容易满足发展成核心对象的条件，样本比较容易聚为同一类；取较小值，则每个样本领域范围较小，不容易成为核心对象，样本容易聚为分散的多个类。同理MinPts衡量一个样本能够成为核心对象的容易程度，若MinPts选取值较大，则每个样本领域都不容易满足包含这么多个其他样本的条件，故很难成为核心对象，样本容易聚为分散的多个类；取较小值，则容易满足条件，样本比较容易聚为同一类。
















\end{document}